[["fp.html", "Introduction Functional programming languages Functional style", " Introduction R, at its heart, is a functional language. This means that it has certain technical properties, but more importantly that it lends itself to a style of problem solving centred on functions. Below I‚Äôll give a brief overview of the technical definition of a functional language, but in this book I will primarily focus on the functional style of programming, because I think it is an extremely good fit to the types of problem you commonly encounter when doing data analysis. Recently, functional techniques have experienced a surge in interest because they can produce efficient and elegant solutions to many modern problems. A functional style tends to create functions that can easily be analysed in isolation (i.e.¬†using only local information), and hence is often much easier to automatically optimise or parallelise. The traditional weaknesses of functional languages, poorer performance and sometimes unpredictable memory usage, have been much reduced in recent years. Functional programming is complementary to object-oriented programming, which has been the dominant programming paradigm for the last several decades. Functional programming languages Every programming language has functions, so what makes a programming language functional? There are many definitions for precisely what makes a language functional, but there are two common threads. Firstly, functional languages have first-class functions, functions that behave like any other data structure. In R, this means that you can do many of the things with a function that you can do with a vector: you can assign them to variables, store them in lists, pass them as arguments to other functions, create them inside functions, and even return them as the result of a function. Secondly, many functional languages require functions to be pure. A function is pure if it satisfies two properties: The output only depends on the inputs, i.e.¬†if you call it again with the same inputs, you get the same outputs. This excludes functions like runif(), read.csv(), or Sys.time() that can return different values. The function has no side-effects, like changing the value of a global variable, writing to disk, or displaying to the screen. This excludes functions like print(), write.csv() and &lt;-. Pure functions are much easier to reason about, but obviously have significant downsides: imagine doing a data analysis where you couldn‚Äôt generate random numbers or read files from disk. Strictly speaking, R isn‚Äôt a functional programming language because it doesn‚Äôt require that you write pure functions. However, you can certainly adopt a functional style in parts of your code: you don‚Äôt have to write pure functions, but you often should. In my experience, partitioning code into functions that are either extremely pure or extremely impure tends to lead to code that is easier to understand and extends to new situations. Functional style It‚Äôs hard to describe exactly what a functional style is, but generally I think it means decomposing a big problem into smaller pieces, then solving each piece with a function or combination of functions. When using a functional style, you strive to decompose components of the problem into isolated functions that operate independently. Each function taken by itself is simple and straightforward to understand; complexity is handled by composing functions in various ways. The following three chapters discuss the three key functional techniques that help you to decompose problems into smaller pieces: Chapter 1 shows you how to replace many for loops with functionals which are functions (like lapply()) that take another function as an argument. Functionals allow you to take a function that solves the problem for a single input and generalise it to handle any number of inputs. Functionals are by far and away the most important technique and you‚Äôll use them all the time in data analysis. Chapter 2 introduces function factories: functions that create functions. Function factories are less commonly used than functionals, but can allow you to elegantly partition work between different parts of your code. Chapter 3 shows you how to create function operators: functions that take functions as input and produce functions as output. They are like adverbs, because they typically modify the operation of a function. Collectively, these types of function are called higher-order functions and they fill out a two-by-two table: "],["functionals.html", "1 Functionals 1.1 Introduction 1.2 My first functional: map() 1.3 Purrr style 1.4 Map variants 1.5 Reduce family 1.6 Predicate functionals 1.7 Base functionals", " 1 Functionals 1.1 Introduction To become significantly more reliable, code must become more transparent. In particular, nested conditions and loops must be viewed with great suspicion. Complicated control flows confuse programmers. Messy code often hides bugs. ‚Äî Bjarne Stroustrup A functional is a function that takes a function as an input and returns a vector as output. Here‚Äôs a simple functional: it calls the function provided as input with 1000 random uniform numbers. randomise &lt;- function(f) f(runif(1e3)) randomise(mean) #&gt; [1] 0.506 randomise(mean) #&gt; [1] 0.501 randomise(sum) #&gt; [1] 489 The chances are that you‚Äôve already used a functional. You might have used for-loop replacements like base R‚Äôs lapply(), apply(), and tapply(); or purrr‚Äôs map(); or maybe you‚Äôve used a mathematical functional like integrate() or optim(). A common use of functionals is as an alternative to for loops. For loops have a bad rap in R because many people believe they are slow1, but the real downside of for loops is that they‚Äôre very flexible: a loop conveys that you‚Äôre iterating, but not what should be done with the results. Just as it‚Äôs better to use while than repeat, and it‚Äôs better to use for than while (Section ??), it‚Äôs better to use a functional than for. Each functional is tailored for a specific task, so when you recognise the functional you immediately know why it‚Äôs being used. If you‚Äôre an experienced for loop user, switching to functionals is typically a pattern matching exercise. You look at the for loop and find a functional that matches the basic form. If one doesn‚Äôt exist, don‚Äôt try and torture an existing functional to fit the form you need. Instead, just leave it as a for loop! (Or once you‚Äôve repeated the same loop two or more times, maybe think about writing your own functional). Outline Section 1.2 introduces your first functional: purrr::map(). Section 1.3 demonstrates how you can combine multiple simple functionals to solve a more complex problem and discusses how purrr style differs from other approaches. Section 1.4 teaches you about 18 (!!) important variants of purrr::map(). Fortunately, their orthogonal design makes them easy to learn, remember, and master. Section 1.5 introduces a new style of functional: purrr::reduce(). reduce() systematically reduces a vector to a single result by applying a function that takes two inputs. Section 1.6 teaches you about predicates: functions that return a single TRUE or FALSE, and the family of functionals that use them to solve common problems. Section 1.7 reviews some functionals in base R that are not members of the map, reduce, or predicate families. Prerequisites This chapter will focus on functionals provided by the purrr package [@purrr]. These functions have a consistent interface that makes it easier to understand the key ideas than their base equivalents, which have grown organically over many years. I‚Äôll compare and contrast base R functions as we go, and then wrap up the chapter with a discussion of base functionals that don‚Äôt have purrr equivalents. library(purrr) 1.2 My first functional: map() The most fundamental functional is purrr::map()2. It takes a vector and a function, calls the function once for each element of the vector, and returns the results in a list. In other words, map(1:3, f) is equivalent to list(f(1), f(2), f(3)). triple &lt;- function(x) x * 3 map(1:3, triple) #&gt; [[1]] #&gt; [1] 3 #&gt; #&gt; [[2]] #&gt; [1] 6 #&gt; #&gt; [[3]] #&gt; [1] 9 Or, graphically: You might wonder why this function is called map(). What does it have to do with depicting physical features of land or sea üó∫? In fact, the meaning comes from mathematics where map refers to ‚Äúan operation that associates each element of a given set with one or more elements of a second set‚Äù. This makes sense here because map() defines a mapping from one vector to another. (‚ÄúMap‚Äù also has the nice property of being short, which is useful for such a fundamental building block.) The implementation of map() is quite simple. We allocate a list the same length as the input, and then fill in the list with a for loop. The heart of the implementation is only a handful of lines of code: simple_map &lt;- function(x, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], ...) } out } The real purrr::map() function has a few differences: it is written in C to eke out every last iota of performance, preserves names, and supports a few shortcuts that you‚Äôll learn about in Section 1.2.2. The base equivalent to map() is lapply(). The only difference is that lapply() does not support the helpers that you‚Äôll learn about below, so if you‚Äôre only using map() from purrr, you can skip the additional dependency and use lapply() directly. 1.2.1 Producing atomic vectors map() returns a list, which makes it the most general of the map family because you can put anything in a list. But it is inconvenient to return a list when a simpler data structure would do, so there are four more specific variants: map_lgl(), map_int(), map_dbl(), and map_chr(). Each returns an atomic vector of the specified type: # map_chr() always returns a character vector map_chr(mtcars, typeof) #&gt; mpg cyl disp hp drat wt qsec vs #&gt; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; &quot;double&quot; #&gt; am gear carb #&gt; &quot;double&quot; &quot;double&quot; &quot;double&quot; # map_lgl() always returns a logical vector map_lgl(mtcars, is.double) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE # map_int() always returns a integer vector n_unique &lt;- function(x) length(unique(x)) map_int(mtcars, n_unique) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 # map_dbl() always returns a double vector map_dbl(mtcars, mean) #&gt; mpg cyl disp hp drat wt qsec vs am gear #&gt; 20.091 6.188 230.722 146.688 3.597 3.217 17.849 0.438 0.406 3.688 #&gt; carb #&gt; 2.812 purrr uses the convention that suffixes, like _dbl(), refer to the output. All map_*() functions can take any type of vector as input. These examples rely on two facts: mtcars is a data frame, and data frames are lists containing vectors of the same length. This is more obvious if we draw a data frame with the same orientation as vector: All map functions always return an output vector the same length as the input, which implies that each call to .f must return a single value. If it does not, you‚Äôll get an error: pair &lt;- function(x) c(x, x) map_dbl(1:2, pair) #&gt; Error: Result 1 must be a single double, not an integer vector of length 2 This is similar to the error you‚Äôll get if .f returns the wrong type of result: map_dbl(1:2, as.character) #&gt; Error: Can&#39;t coerce element 1 from a character to a double In either case, it‚Äôs often useful to switch back to map(), because map() can accept any type of output. That allows you to see the problematic output, and figure out what to do with it. map(1:2, pair) #&gt; [[1]] #&gt; [1] 1 1 #&gt; #&gt; [[2]] #&gt; [1] 2 2 map(1:2, as.character) #&gt; [[1]] #&gt; [1] &quot;1&quot; #&gt; #&gt; [[2]] #&gt; [1] &quot;2&quot; Base R has two apply functions that can return atomic vectors: sapply() and vapply(). I recommend that you avoid sapply() because it tries to simplify the result, so it can return a list, a vector, or a matrix. This makes it difficult to program with, and it should be avoided in non-interactive settings. vapply() is safer because it allows you to provide a template, FUN.VALUE, that describes the output shape. If you don‚Äôt want to use purrr, I recommend you always use vapply() in your functions, not sapply(). The primary downside of vapply() is its verbosity: for example, the equivalent to map_dbl(x, mean, na.rm = TRUE) is vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1)). 1.2.2 Anonymous functions and shortcuts Instead of using map() with an existing function, you can create an inline anonymous function (as mentioned in Section ??): map_dbl(mtcars, function(x) length(unique(x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 Anonymous functions are very useful, but the syntax is verbose. So purrr supports a special shortcut: map_dbl(mtcars, ~ length(unique(.x))) #&gt; mpg cyl disp hp drat wt qsec vs am gear carb #&gt; 25 3 27 22 22 29 30 2 2 3 6 This works because all purrr functions translate formulas, created by ~ (pronounced ‚Äútwiddle‚Äù), into functions. You can see what‚Äôs happening behind the scenes by calling as_mapper(): as_mapper(~ length(unique(.x))) #&gt; &lt;lambda&gt; #&gt; function (..., .x = ..1, .y = ..2, . = ..1) #&gt; length(unique(.x)) #&gt; attr(,&quot;class&quot;) #&gt; [1] &quot;rlang_lambda_function&quot; &quot;function&quot; The function arguments look a little quirky but allow you to refer to . for one argument functions, .x and .y for two argument functions, and ..1, ..2, ..3, etc, for functions with an arbitrary number of arguments. . remains for backward compatibility but I don‚Äôt recommend using it because it‚Äôs easily confused with the . used by magrittr‚Äôs pipe. This shortcut is particularly useful for generating random data: x &lt;- map(1:3, ~ runif(2)) str(x) #&gt; List of 3 #&gt; $ : num [1:2] 0.281 0.53 #&gt; $ : num [1:2] 0.433 0.917 #&gt; $ : num [1:2] 0.0275 0.8249 Reserve this syntax for short and simple functions. A good rule of thumb is that if your function spans lines or uses {}, it‚Äôs time to give it a name. The map functions also have shortcuts for extracting elements from a vector, powered by purrr::pluck(). You can use a character vector to select elements by name, an integer vector to select by position, or a list to select by both name and position. These are very useful for working with deeply nested lists, which often arise when working with JSON. x &lt;- list( list(-1, x = 1, y = c(2), z = &quot;a&quot;), list(-2, x = 4, y = c(5, 6), z = &quot;b&quot;), list(-3, x = 8, y = c(9, 10, 11)) ) # Select by name map_dbl(x, &quot;x&quot;) #&gt; [1] 1 4 8 # Or by position map_dbl(x, 1) #&gt; [1] -1 -2 -3 # Or by both map_dbl(x, list(&quot;y&quot;, 1)) #&gt; [1] 2 5 9 # You&#39;ll get an error if a component doesn&#39;t exist: map_chr(x, &quot;z&quot;) #&gt; Error: Result 3 must be a single string, not NULL of length 0 # Unless you supply a .default value map_chr(x, &quot;z&quot;, .default = NA) #&gt; [1] &quot;a&quot; &quot;b&quot; NA In base R functions, like lapply(), you can provide the name of the function as a string. This isn‚Äôt tremendously useful as lapply(x, \"f\") is almost always equivalent to lapply(x, f) and is more typing. 1.2.3 Passing arguments with ... It‚Äôs often convenient to pass along additional arguments to the function that you‚Äôre calling. For example, you might want to pass na.rm = TRUE along to mean(). One way to do that is with an anonymous function: x &lt;- list(1:5, c(1:10, NA)) map_dbl(x, ~ mean(.x, na.rm = TRUE)) #&gt; [1] 3.0 5.5 But because the map functions pass ... along, there‚Äôs a simpler form available: map_dbl(x, mean, na.rm = TRUE) #&gt; [1] 3.0 5.5 This is easiest to understand with a picture: any arguments that come after f in the call to map() are inserted after the data in individual calls to f(): It‚Äôs important to note that these arguments are not decomposed; or said another way, map() is only vectorised over its first argument. If an argument after f is a vector, it will be passed along as is: (You‚Äôll learn about map variants that are vectorised over multiple arguments in Sections 1.4.2 and 1.4.5.) Note there‚Äôs a subtle difference between placing extra arguments inside an anonymous function compared with passing them to map(). Putting them in an anonymous function means that they will be evaluated every time f() is executed, not just once when you call map(). This is easiest to see if we make the additional argument random: plus &lt;- function(x, y) x + y x &lt;- c(0, 0, 0, 0) map_dbl(x, plus, runif(1)) #&gt; [1] 0.0625 0.0625 0.0625 0.0625 map_dbl(x, ~ plus(.x, runif(1))) #&gt; [1] 0.903 0.132 0.629 0.945 1.2.4 Argument names In the diagrams, I‚Äôve omitted argument names to focus on the overall structure. But I recommend writing out the full names in your code, as it makes it easier to read. map(x, mean, 0.1) is perfectly valid code, but will call mean(x[[1]], 0.1) so it relies on the reader remembering that the second argument to mean() is trim. To avoid unnecessary burden on the brain of the reader3, be kind and write map(x, mean, trim = 0.1). This is the reason why the arguments to map() are a little odd: instead of being x and f, they are .x and .f. It‚Äôs easiest to see the problem that leads to these names using simple_map() defined above. simple_map() has arguments x and f so you‚Äôll have problems whenever the function you are calling has arguments x or f: boostrap_summary &lt;- function(x, f) { f(sample(x, replace = TRUE)) } simple_map(mtcars, boostrap_summary, f = mean) #&gt; Error in mean.default(x[[i]], ...): &#39;trim&#39; must be numeric of length one The error is a little bewildering until you remember that the call to simple_map() is equivalent to simple_map(x = mtcars, f = mean, bootstrap_summary) because named matching beats positional matching. purrr functions reduce the likelihood of such a clash by using .f and .x instead of the more common f and x. Of course this technique isn‚Äôt perfect (because the function you are calling might still use .f and .x), but it avoids 99% of issues. The remaining 1% of the time, use an anonymous function. Base functions that pass along ... use a variety of naming conventions to prevent undesired argument matching: The apply family mostly uses capital letters (e.g.¬†X and FUN). transform() uses the more exotic prefix _: this makes the name non-syntactic so it must always be surrounded in `, as described in Section ??. This makes undesired matches extremely unlikely. Other functionals like uniroot() and optim() make no effort to avoid clashes but they tend to be used with specially created functions so clashes are less likely. 1.2.5 Varying another argument So far the first argument to map() has always become the first argument to the function. But what happens if the first argument should be constant, and you want to vary a different argument? How do you get the result in this picture? It turns out that there‚Äôs no way to do it directly, but there are two tricks you can use instead. To illustrate them, imagine I have a vector that contains a few unusual values, and I want to explore the effect of different amounts of trimming when computing the mean. In this case, the first argument to mean() will be constant, and I want to vary the second argument, trim. trims &lt;- c(0, 0.1, 0.2, 0.5) x &lt;- rcauchy(1000) The simplest technique is to use an anonymous function to rearrange the argument order: map_dbl(trims, ~ mean(x, trim = .x)) #&gt; [1] -0.3500 0.0434 0.0354 0.0502 This is still a little confusing because I‚Äôm using both x and .x. You can make it a little clearer by abandoning the ~ helper: map_dbl(trims, function(trim) mean(x, trim = trim)) #&gt; [1] -0.3500 0.0434 0.0354 0.0502 Sometimes, if you want to be (too) clever, you can take advantage of R‚Äôs flexible argument matching rules (as described in Section ??). For example, in this example you can rewrite mean(x, trim = 0.1) as mean(0.1, x = x), so you could write the call to map_dbl() as: map_dbl(trims, mean, x = x) #&gt; [1] -0.3500 0.0434 0.0354 0.0502 I don‚Äôt recommend this technique as it relies on the reader‚Äôs familiarity with both the argument order to .f, and R‚Äôs argument matching rules. You‚Äôll see one more alternative in Section 1.4.5. 1.2.6 Exercises Use as_mapper() to explore how purrr generates anonymous functions for the integer, character, and list helpers. What helper allows you to extract attributes? Read the documentation to find out. map(1:3, ~ runif(2)) is a useful pattern for generating random numbers, but map(1:3, runif(2)) is not. Why not? Can you explain why it returns the result that it does? Use the appropriate map() function to: Compute the standard deviation of every column in a numeric data frame. Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you‚Äôll need to do it in two steps.) Compute the number of levels for every factor in a data frame. The following code simulates the performance of a t-test for non-normal data. Extract the p-value from each test, then visualise. trials &lt;- map(1:100, ~ t.test(rpois(10, 10), rpois(7, 10))) The following code uses a map nested inside another map to apply a function to every element of a nested list. Why does it fail, and what do you need to do to make it work? x &lt;- list( list(1, c(3, 9)), list(c(3, 6), 7, c(4, 7, 6)) ) triple &lt;- function(x) x * 3 map(x, map, .f = triple) #&gt; Error in .f(.x[[i]], ...): unused argument (function (.x, .f, ...) #&gt; { #&gt; .f &lt;- as_mapper(.f, ...) #&gt; .Call(map_impl, environment(), &quot;.x&quot;, &quot;.f&quot;, &quot;list&quot;) #&gt; }) Use map() to fit linear models to the mtcars dataset using the formulas stored in this list: formulas &lt;- list( mpg ~ disp, mpg ~ I(1 / disp), mpg ~ disp + wt, mpg ~ I(1 / disp) + wt ) Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below, then extract the \\(R^2\\) of the model fit (Hint: you can compute the \\(R^2\\) with summary().) bootstrap &lt;- function(df) { df[sample(nrow(df), replace = TRUE), , drop = FALSE] } bootstraps &lt;- map(1:10, ~ bootstrap(mtcars)) 1.3 Purrr style Before we go on to explore more map variants, let‚Äôs take a quick look at how you tend to use multiple purrr functions to solve a moderately realistic problem: fitting a model to each subgroup and extracting a coefficient of the model. For this toy example, I‚Äôm going to break the mtcars data set down into groups defined by the number of cylinders, using the base split function: by_cyl &lt;- split(mtcars, mtcars$cyl) This creates a list of three data frames: the cars with 4, 6, and 8 cylinders respectively. Now imagine we want to fit a linear model, then extract the second coefficient (i.e.¬†the slope). The following code shows how you might do that with purrr: by_cyl %&gt;% map(~ lm(mpg ~ wt, data = .x)) %&gt;% map(coef) %&gt;% map_dbl(2) #&gt; 4 6 8 #&gt; -5.65 -2.78 -2.19 (If you haven‚Äôt seen %&gt;%, the pipe, before, it‚Äôs described in Section ??.) I think this code is easy to read because each line encapsulates a single step, you can easily distinguish the functional from what it does, and the purrr helpers allow us to very concisely describe what to do in each step. How would you attack this problem with base R? You certainly could replace each purrr function with the equivalent base function: by_cyl %&gt;% lapply(function(data) lm(mpg ~ wt, data = data)) %&gt;% lapply(coef) %&gt;% vapply(function(x) x[[2]], double(1)) #&gt; 4 6 8 #&gt; -5.65 -2.78 -2.19 But this isn‚Äôt really base R since we‚Äôre using the pipe. To tackle purely in base I think you‚Äôd use an intermediate variable, and do more in each step: models &lt;- lapply(by_cyl, function(data) lm(mpg ~ wt, data = data)) vapply(models, function(x) coef(x)[[2]], double(1)) #&gt; 4 6 8 #&gt; -5.65 -2.78 -2.19 Or, of course, you could use a for loop: slopes &lt;- double(length(by_cyl)) for (i in seq_along(by_cyl)) { model &lt;- lm(mpg ~ wt, data = by_cyl[[i]]) slopes[[i]] &lt;- coef(model)[[2]] } slopes #&gt; [1] -5.65 -2.78 -2.19 It‚Äôs interesting to note that as you move from purrr to base apply functions to for loops you tend to do more and more in each iteration. In purrr we iterate 3 times (map(), map(), map_dbl()), with apply functions we iterate twice (lapply(), vapply()), and with a for loop we iterate once. I prefer more, but simpler, steps because I think it makes the code easier to understand and later modify. 1.4 Map variants There are 23 primary variants of map(). So far, you‚Äôve learned about five (map(), map_lgl(), map_int(), map_dbl() and map_chr()). That means that you‚Äôve got 18 (!!) more to learn. That sounds like a lot, but fortunately the design of purrr means that you only need to learn five new ideas: Output same type as input with modify() Iterate over two inputs with map2(). Iterate with an index using imap() Return nothing with walk(). Iterate over any number of inputs with pmap(). The map family of functions has orthogonal input and outputs, meaning that we can organise all the family into a matrix, with inputs in the rows and outputs in the columns. Once you‚Äôve mastered the idea in a row, you can combine it with any column; once you‚Äôve mastered the idea in a column, you can combine it with any row. That relationship is summarised in the following table: List Atomic Same type Nothing One argument map() map_lgl(), ‚Ä¶ modify() walk() Two arguments map2() map2_lgl(), ‚Ä¶ modify2() walk2() One argument + index imap() imap_lgl(), ‚Ä¶ imodify() iwalk() N arguments pmap() pmap_lgl(), ‚Ä¶ ‚Äî pwalk() 1.4.1 Same type of output as input: modify() Imagine you wanted to double every column in a data frame. You might first try using map(), but map() always returns a list: df &lt;- data.frame( x = 1:3, y = 6:4 ) map(df, ~ .x * 2) #&gt; $x #&gt; [1] 2 4 6 #&gt; #&gt; $y #&gt; [1] 12 10 8 If you want to keep the output as a data frame, you can use modify(), which always returns the same type of output as the input: modify(df, ~ .x * 2) #&gt; x y #&gt; 1 2 12 #&gt; 2 4 10 #&gt; 3 6 8 Despite the name, modify() doesn‚Äôt modify in place, it returns a modified copy, so if you wanted to permanently modify df, you‚Äôd need to assign it: df &lt;- modify(df, ~ .x * 2) As usual, the basic implementation of modify() is simple, and in fact it‚Äôs even simpler than map() because we don‚Äôt need to create a new output vector; we can just progressively replace the input. (The real code is a little complex to handle edge cases more gracefully.) simple_modify &lt;- function(x, f, ...) { for (i in seq_along(x)) { x[[i]] &lt;- f(x[[i]], ...) } x } In Section 1.6.2 you‚Äôll learn about a very useful variant of modify(), called modify_if(). This allows you to (e.g.) only double numeric columns of a data frame with modify_if(df, is.numeric, ~ .x * 2). 1.4.2 Two inputs: map2() and friends map() is vectorised over a single argument, .x. This means it only varies .x when calling .f, and all other arguments are passed along unchanged, thus making it poorly suited for some problems. For example, how would you find a weighted mean when you have a list of observations and a list of weights? Imagine we have the following data: xs &lt;- map(1:8, ~ runif(10)) xs[[1]][[1]] &lt;- NA ws &lt;- map(1:8, ~ rpois(10, 5) + 1) You can use map_dbl() to compute the unweighted means: map_dbl(xs, mean) #&gt; [1] NA 0.463 0.551 0.453 0.564 0.501 0.371 0.443 But passing ws as an additional argument doesn‚Äôt work because arguments after .f are not transformed: map_dbl(xs, weighted.mean, w = ws) #&gt; Error in weighted.mean.default(.x[[i]], ...): &#39;x&#39; and &#39;w&#39; must have the same length We need a new tool: a map2(), which is vectorised over two arguments. This means both .x and .y are varied in each call to .f: map2_dbl(xs, ws, weighted.mean) #&gt; [1] NA 0.451 0.603 0.452 0.563 0.510 0.342 0.464 The arguments to map2() are slightly different to the arguments to map() as two vectors come before the function, rather than one. Additional arguments still go afterwards: map2_dbl(xs, ws, weighted.mean, na.rm = TRUE) #&gt; [1] 0.504 0.451 0.603 0.452 0.563 0.510 0.342 0.464 The basic implementation of map2() is simple, and quite similar to that of map(). Instead of iterating over one vector, we iterate over two in parallel: simple_map2 &lt;- function(x, y, f, ...) { out &lt;- vector(&quot;list&quot;, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- f(x[[i]], y[[i]], ...) } out } One of the big differences between map2() and the simple function above is that map2() recycles its inputs to make sure that they‚Äôre the same length: In other words, map2(x, y, f) will automatically behave like map(x, f, y) when needed. This is helpful when writing functions; in scripts you‚Äôd generally just use the simpler form directly. The closest base equivalent to map2() is Map(), which is discussed in Section 1.4.5. 1.4.3 No outputs: walk() and friends Most functions are called for the value that they return, so it makes sense to capture and store the value with a map() function. But some functions are called primarily for their side-effects (e.g.¬†cat(), write.csv(), or ggsave()) and it doesn‚Äôt make sense to capture their results. Take this simple example that displays a welcome message using cat(). cat() returns NULL, so while map() works (in the sense that it generates the desired welcomes), it also returns list(NULL, NULL). welcome &lt;- function(x) { cat(&quot;Welcome &quot;, x, &quot;!\\n&quot;, sep = &quot;&quot;) } names &lt;- c(&quot;Hadley&quot;, &quot;Jenny&quot;) # As well as generate the welcomes, it also shows # the return value of cat() map(names, welcome) #&gt; Welcome Hadley! #&gt; Welcome Jenny! #&gt; [[1]] #&gt; NULL #&gt; #&gt; [[2]] #&gt; NULL You could avoid this problem by assigning the results of map() to a variable that you never use, but that would muddy the intent of the code. Instead, purrr provides the walk family of functions that ignore the return values of the .f and instead return .x invisibly4. walk(names, welcome) #&gt; Welcome Hadley! #&gt; Welcome Jenny! My visual depiction of walk attempts to capture the important difference from map(): the outputs are ephemeral, and the input is returned invisibly. One of the most useful walk() variants is walk2() because a very common side-effect is saving something to disk, and when saving something to disk you always have a pair of values: the object and the path that you want to save it to. For example, imagine you have a list of data frames (which I‚Äôve created here using split()), and you‚Äôd like to save each one to a separate CSV file. That‚Äôs easy with walk2(): temp &lt;- tempfile() dir.create(temp) cyls &lt;- split(mtcars, mtcars$cyl) paths &lt;- file.path(temp, paste0(&quot;cyl-&quot;, names(cyls), &quot;.csv&quot;)) walk2(cyls, paths, write.csv) dir(temp) #&gt; [1] &quot;cyl-4.csv&quot; &quot;cyl-6.csv&quot; &quot;cyl-8.csv&quot; Here the walk2() is equivalent to write.csv(cyls[[1]], paths[[1]]), write.csv(cyls[[2]], paths[[2]]), write.csv(cyls[[3]], paths[[3]]). There is no base equivalent to walk(); either wrap the result of lapply() in invisible() or save it to a variable that is never used. 1.4.4 Iterating over values and indices There are three basic ways to loop over a vector with a for loop: Loop over the elements: for (x in xs) Loop over the numeric indices: for (i in seq_along(xs)) Loop over the names: for (nm in names(xs)) The first form is analogous to the map() family. The second and third forms are equivalent to the imap() family which allows you to iterate over the values and the indices of a vector in parallel. imap() is like map2() in the sense that your .f gets called with two arguments, but here both are derived from the vector. imap(x, f) is equivalent to map2(x, names(x), f) if x has names, and map2(x, seq_along(x), f) if it does not. imap() is often useful for constructing labels: imap_chr(iris, ~ paste0(&quot;The first value of &quot;, .y, &quot; is &quot;, .x[[1]])) #&gt; Sepal.Length #&gt; &quot;The first value of Sepal.Length is 5.1&quot; #&gt; Sepal.Width #&gt; &quot;The first value of Sepal.Width is 3.5&quot; #&gt; Petal.Length #&gt; &quot;The first value of Petal.Length is 1.4&quot; #&gt; Petal.Width #&gt; &quot;The first value of Petal.Width is 0.2&quot; #&gt; Species #&gt; &quot;The first value of Species is setosa&quot; If the vector is unnamed, the second argument will be the index: x &lt;- map(1:6, ~ sample(1000, 10)) imap_chr(x, ~ paste0(&quot;The highest value of &quot;, .y, &quot; is &quot;, max(.x))) #&gt; [1] &quot;The highest value of 1 is 975&quot; &quot;The highest value of 2 is 915&quot; #&gt; [3] &quot;The highest value of 3 is 982&quot; &quot;The highest value of 4 is 955&quot; #&gt; [5] &quot;The highest value of 5 is 971&quot; &quot;The highest value of 6 is 696&quot; imap() is a useful helper if you want to work with the values in a vector along with their positions. 1.4.5 Any number of inputs: pmap() and friends Since we have map() and map2(), you might expect map3(), map4(), map5(), ‚Ä¶ But where would you stop? Instead of generalising map2() to an arbitrary number of arguments, purrr takes a slightly different tack with pmap(): you supply it a single list, which contains any number of arguments. In most cases, that will be a list of equal-length vectors, i.e.¬†something very similar to a data frame. In diagrams, I‚Äôll emphasise that relationship by drawing the input similar to a data frame. There‚Äôs a simple equivalence between map2() and pmap(): map2(x, y, f) is the same as pmap(list(x, y), f). The pmap() equivalent to the map2_dbl(xs, ws, weighted.mean) used above is: pmap_dbl(list(xs, ws), weighted.mean) #&gt; [1] NA 0.451 0.603 0.452 0.563 0.510 0.342 0.464 As before, the varying arguments come before .f (although now they must be wrapped in a list), and the constant arguments come afterwards. pmap_dbl(list(xs, ws), weighted.mean, na.rm = TRUE) #&gt; [1] 0.504 0.451 0.603 0.452 0.563 0.510 0.342 0.464 A big difference between pmap() and the other map functions is that pmap() gives you much finer control over argument matching because you can name the components of the list. Returning to our example from Section 1.2.5, where we wanted to vary the trim argument to x, we could instead use pmap(): trims &lt;- c(0, 0.1, 0.2, 0.5) x &lt;- rcauchy(1000) pmap_dbl(list(trim = trims), mean, x = x) #&gt; [1] -6.6740 0.0210 0.0235 0.0151 I think it‚Äôs good practice to name the components of the list to make it very clear how the function will be called. It‚Äôs often convenient to call pmap() with a data frame. A handy way to create that data frame is with tibble::tribble(), which allows you to describe a data frame row-by-row (rather than column-by-column, as usual): thinking about the parameters to a function as a data frame is a very powerful pattern. The following example shows how you might draw random uniform numbers with varying parameters: params &lt;- tibble::tribble( ~ n, ~ min, ~ max, 1L, 0, 1, 2L, 10, 100, 3L, 100, 1000 ) pmap(params, runif) #&gt; [[1]] #&gt; [1] 0.332 #&gt; #&gt; [[2]] #&gt; [1] 53.5 47.6 #&gt; #&gt; [[3]] #&gt; [1] 231 715 515 Here, the column names are critical: I‚Äôve carefully chosen to match them to the arguments to runif(), so the pmap(params, runif) is equivalent to runif(n = 1L, min = 0, max = 1), runif(n = 2, min = 10, max = 100), runif(n = 3L, min = 100, max = 1000). (If you have a data frame in hand, and the names don‚Äôt match, use dplyr::rename() or similar.) There are two base equivalents to the pmap() family: Map() and mapply(). Both have significant drawbacks: Map() vectorises over all arguments so you cannot supply arguments that do not vary. mapply() is the multidimensional version of sapply(); conceptually it takes the output of Map() and simplifies it if possible. This gives it similar issues to sapply(). There is no multi-input equivalent of vapply(). 1.4.6 Exercises Explain the results of modify(mtcars, 1). Rewrite the following code to use iwalk() instead of walk2(). What are the advantages and disadvantages? cyls &lt;- split(mtcars, mtcars$cyl) paths &lt;- file.path(temp, paste0(&quot;cyl-&quot;, names(cyls), &quot;.csv&quot;)) walk2(cyls, paths, write.csv) Explain how the following code transforms a data frame using functions stored in a list. trans &lt;- list( disp = function(x) x * 0.0163871, am = function(x) factor(x, labels = c(&quot;auto&quot;, &quot;manual&quot;)) ) nm &lt;- names(trans) mtcars[nm] &lt;- map2(trans, mtcars[nm], function(f, var) f(var)) Compare and contrast the map2() approach to this map() approach: mtcars[nm] &lt;- map(nm, ~ trans[[.x]](mtcars[[.x]])) What does write.csv() return, i.e.¬†what happens if you use it with map2() instead of walk2()? 1.5 Reduce family After the map family, the next most important family of functions is the reduce family. This family is much smaller, with only two main variants, and is used less commonly, but it‚Äôs a powerful idea, gives us the opportunity to discuss some useful algebra, and powers the map-reduce framework frequently used for processing very large datasets. 1.5.1 Basics reduce() takes a vector of length n and produces a vector of length 1 by calling a function with a pair of values at a time: reduce(1:4, f) is equivalent to f(f(f(1, 2), 3), 4). reduce() is a useful way to generalise a function that works with two inputs (a binary function) to work with any number of inputs. Imagine you have a list of numeric vectors, and you want to find the values that occur in every element. First we generate some sample data: l &lt;- map(1:4, ~ sample(1:10, 15, replace = T)) str(l) #&gt; List of 4 #&gt; $ : int [1:15] 7 1 8 8 3 8 2 4 7 10 ... #&gt; $ : int [1:15] 3 1 10 2 5 2 9 8 5 4 ... #&gt; $ : int [1:15] 6 10 9 5 6 7 8 6 10 8 ... #&gt; $ : int [1:15] 9 8 6 4 4 5 2 9 9 6 ... To solve this challenge we need to use intersect() repeatedly: out &lt;- l[[1]] out &lt;- intersect(out, l[[2]]) out &lt;- intersect(out, l[[3]]) out &lt;- intersect(out, l[[4]]) out #&gt; [1] 8 4 reduce() automates this solution for us, so we can write: reduce(l, intersect) #&gt; [1] 8 4 We could apply the same idea if we wanted to list all the elements that appear in at least one entry. All we have to do is switch from intersect() to union(): reduce(l, union) #&gt; [1] 7 1 8 3 2 4 10 5 9 6 Like the map family, you can also pass additional arguments. intersect() and union() don‚Äôt take extra arguments so I can‚Äôt demonstrate them here, but the principle is straightforward and I drew you a picture. As usual, the essence of reduce() can be reduced to a simple wrapper around a for loop: simple_reduce &lt;- function(x, f) { out &lt;- x[[1]] for (i in seq(2, length(x))) { out &lt;- f(out, x[[i]]) } out } The base equivalent is Reduce(). Note that the argument order is different: the function comes first, followed by the vector, and there is no way to supply additional arguments. 1.5.2 Accumulate The first reduce() variant, accumulate(), is useful for understanding how reduce works, because instead of returning just the final result, it returns all the intermediate results as well: accumulate(l, intersect) #&gt; [[1]] #&gt; [1] 7 1 8 8 3 8 2 4 7 10 10 3 7 10 10 #&gt; #&gt; [[2]] #&gt; [1] 1 8 3 2 4 10 #&gt; #&gt; [[3]] #&gt; [1] 8 4 10 #&gt; #&gt; [[4]] #&gt; [1] 8 4 Another useful way to understand reduce is to think about sum(): sum(x) is equivalent to x[[1]] + x[[2]] + x[[3]] + ..., i.e.¬†reduce(x, `+`). Then accumulate(x, `+`) is the cumulative sum: x &lt;- c(4, 3, 10) reduce(x, `+`) #&gt; [1] 17 accumulate(x, `+`) #&gt; [1] 4 7 17 1.5.3 Output types In the above example using +, what should reduce() return when x is short, i.e.¬†length 1 or 0? Without additional arguments, reduce() just returns the input when x is length 1: reduce(1, `+`) #&gt; [1] 1 This means that reduce() has no way to check that the input is valid: reduce(&quot;a&quot;, `+`) #&gt; [1] &quot;a&quot; What if it‚Äôs length 0? We get an error that suggests we need to use the .init argument: reduce(integer(), `+`) #&gt; Error: `.x` is empty, and no `.init` supplied What should .init be here? To figure that out, we need to see what happens when .init is supplied: So if we call reduce(1, `+`, init) the result will be 1 + init. Now we know that the result should be just 1, so that suggests that .init should be 0: reduce(integer(), `+`, .init = 0) #&gt; [1] 0 This also ensures that reduce() checks that length 1 inputs are valid for the function that you‚Äôre calling: reduce(&quot;a&quot;, `+`, .init = 0) #&gt; Error in .x + .y: non-numeric argument to binary operator If you want to get algebraic about it, 0 is called the identity of the real numbers under the operation of addition: if you add a 0 to any number, you get the same number back. R applies the same principle to determine what a summary function with a zero length input should return: sum(integer()) # x + 0 = x #&gt; [1] 0 prod(integer()) # x * 1 = x #&gt; [1] 1 min(integer()) # min(x, Inf) = x #&gt; [1] Inf max(integer()) # max(x, -Inf) = x #&gt; [1] -Inf If you‚Äôre using reduce() in a function, you should always supply .init. Think carefully about what your function should return when you pass a vector of length 0 or 1, and make sure to test your implementation. 1.5.4 Multiple inputs Very occasionally you need to pass two arguments to the function that you‚Äôre reducing. For example, you might have a list of data frames that you want to join together, and the variables you use to join will vary from element to element. This is a very specialised scenario, so I don‚Äôt want to spend much time on it, but I do want you to know that reduce2() exists. The length of the second argument varies based on whether or not .init is supplied: if you have four elements of x, f will only be called three times. If you supply init, f will be called four times. 1.5.5 Map-reduce You might have heard of map-reduce, the idea that powers technology like Hadoop. Now you can see how simple and powerful the underlying idea is: map-reduce is a map combined with a reduce. The difference for large data is that the data is spread over multiple computers. Each computer performs the map on the data that it has, then it sends the result to back to a coordinator which reduces the individual results back to a single result. As a simple example, imagine computing the mean of a very large vector, so large that it has to be split over multiple computers. You could ask each computer to calculate the sum and the length, and then return those to the coordinator which computes the overall mean by dividing the total sum by the total length. 1.6 Predicate functionals A predicate is a function that returns a single TRUE or FALSE, like is.character(), is.null(), or all(), and we say a predicate matches a vector if it returns TRUE. 1.6.1 Basics A predicate functional applies a predicate to each element of a vector. purrr provides seven useful functions which come in three groups: some(.x, .p) returns TRUE if any element matches; every(.x, .p) returns TRUE if all elements match; none(.x, .p) returns TRUE if no element matches. These are similar to any(map_lgl(.x, .p)), all(map_lgl(.x, .p)) and all(map_lgl(.x, negate(.p))) but they terminate early: some() returns TRUE when it sees the first TRUE, and every() and none() return FALSE when they see the first FALSE or TRUE respectively. detect(.x, .p) returns the value of the first match; detect_index(.x, .p) returns the location of the first match. keep(.x, .p) keeps all matching elements; discard(.x, .p) drops all matching elements. The following example shows how you might use these functionals with a data frame: df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) detect(df, is.factor) #&gt; NULL detect_index(df, is.factor) #&gt; [1] 0 str(keep(df, is.factor)) #&gt; &#39;data.frame&#39;: 3 obs. of 0 variables str(discard(df, is.factor)) #&gt; &#39;data.frame&#39;: 3 obs. of 2 variables: #&gt; $ x: int 1 2 3 #&gt; $ y: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; 1.6.2 Map variants map() and modify() come in variants that also take predicate functions, transforming only the elements of .x where .p is TRUE. df &lt;- data.frame( num1 = c(0, 10, 20), num2 = c(5, 6, 7), chr1 = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), stringsAsFactors = FALSE ) str(map_if(df, is.numeric, mean)) #&gt; List of 3 #&gt; $ num1: num 10 #&gt; $ num2: num 6 #&gt; $ chr1: chr [1:3] &quot;a&quot; &quot;b&quot; &quot;c&quot; str(modify_if(df, is.numeric, mean)) #&gt; &#39;data.frame&#39;: 3 obs. of 3 variables: #&gt; $ num1: num 10 10 10 #&gt; $ num2: num 6 6 6 #&gt; $ chr1: chr &quot;a&quot; &quot;b&quot; &quot;c&quot; str(map(keep(df, is.numeric), mean)) #&gt; List of 2 #&gt; $ num1: num 10 #&gt; $ num2: num 6 1.6.3 Exercises Why isn‚Äôt is.na() a predicate function? What base R function is closest to being a predicate version of is.na()? simple_reduce() has a problem when x is length 0 or length 1. Describe the source of the problem and how you might go about fixing it. simple_reduce &lt;- function(x, f) { out &lt;- x[[1]] for (i in seq(2, length(x))) { out &lt;- f(out, x[[i]]) } out } Implement the span() function from Haskell: given a list x and a predicate function f, span(x, f) returns the location of the longest sequential run of elements where the predicate is true. (Hint: you might find rle() helpful.) Implement arg_max(). It should take a function and a vector of inputs, and return the elements of the input where the function returns the highest value. For example, arg_max(-10:5, function(x) x ^ 2) should return -10. arg_max(-5:5, function(x) x ^ 2) should return c(-5, 5). Also implement the matching arg_min() function. The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame? How would you apply it to every numeric column in a data frame? scale01 &lt;- function(x) { rng &lt;- range(x, na.rm = TRUE) (x - rng[1]) / (rng[2] - rng[1]) } 1.7 Base functionals To finish up the chapter, here I provide a survey of important base functionals that are not members of the map, reduce, or predicate families, and hence have no equivalent in purrr. This is not to say that they‚Äôre not important, but they have more of a mathematical or statistical flavour, and they are generally less useful in data analysis. 1.7.1 Matrices and arrays map() and friends are specialised to work with one-dimensional vectors. base::apply() is specialised to work with two-dimensional and higher vectors, i.e.¬†matrices and arrays. You can think of apply() as an operation that summarises a matrix or array by collapsing each row or column to a single value. It has four arguments: X, the matrix or array to summarise. MARGIN, an integer vector giving the dimensions to summarise over, 1 = rows, 2 = columns, etc. (The argument name comes from thinking about the margins of a joint distribution.) FUN, a summary function. ... other arguments passed on to FUN. A typical example of apply() looks like this a2d &lt;- matrix(1:20, nrow = 5) apply(a2d, 1, mean) #&gt; [1] 8.5 9.5 10.5 11.5 12.5 apply(a2d, 2, mean) #&gt; [1] 3 8 13 18 You can specify multiple dimensions to MARGIN, which is useful for high-dimensional arrays: a3d &lt;- array(1:24, c(2, 3, 4)) apply(a3d, 1, mean) #&gt; [1] 12 13 apply(a3d, c(1, 2), mean) #&gt; [,1] [,2] [,3] #&gt; [1,] 10 12 14 #&gt; [2,] 11 13 15 There are two caveats to using apply(): Like base::sapply(), you have no control over the output type; it will automatically be simplified to a list, matrix, or vector. However, you usually use apply() with numeric arrays and a numeric summary function so you are less likely to encounter a problem than with sapply(). apply() is also not idempotent in the sense that if the summary function is the identity operator, the output is not always the same as the input. a1 &lt;- apply(a2d, 1, identity) identical(a2d, a1) #&gt; [1] FALSE a2 &lt;- apply(a2d, 2, identity) identical(a2d, a2) #&gt; [1] TRUE Never use apply() with a data frame. It always coerces it to a matrix, which will lead to undesirable results if your data frame contains anything other than numbers. df &lt;- data.frame(x = 1:3, y = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)) apply(df, 2, mean) #&gt; Warning in mean.default(newX[, i], ...): argument is not numeric or logical: #&gt; returning NA #&gt; Warning in mean.default(newX[, i], ...): argument is not numeric or logical: #&gt; returning NA #&gt; x y #&gt; NA NA 1.7.2 Mathematical concerns Functionals are very common in mathematics. The limit, the maximum, the roots (the set of points where f(x) = 0), and the definite integral are all functionals: given a function, they return a single number (or vector of numbers). At first glance, these functions don‚Äôt seem to fit in with the theme of eliminating loops, but if you dig deeper you‚Äôll find out that they are all implemented using an algorithm that involves iteration. Base R provides a useful set: integrate() finds the area under the curve defined by f() uniroot() finds where f() hits zero optimise() finds the location of the lowest (or highest) value of f() The following example shows how functionals might be used with a simple function, sin(): integrate(sin, 0, pi) #&gt; 2 with absolute error &lt; 2.2e-14 str(uniroot(sin, pi * c(1 / 2, 3 / 2))) #&gt; List of 5 #&gt; $ root : num 3.14 #&gt; $ f.root : num 1.22e-16 #&gt; $ iter : int 2 #&gt; $ init.it : int NA #&gt; $ estim.prec: num 6.1e-05 str(optimise(sin, c(0, 2 * pi))) #&gt; List of 2 #&gt; $ minimum : num 4.71 #&gt; $ objective: num -1 str(optimise(sin, c(0, pi), maximum = TRUE)) #&gt; List of 2 #&gt; $ maximum : num 1.57 #&gt; $ objective: num 1 1.7.3 Exercises How does apply() arrange the output? Read the documentation and perform some experiments. What do eapply() and rapply() do? Does purrr have equivalents? Challenge: read about the fixed point algorithm. Complete the exercises using R. Typically it‚Äôs not the for loop itself that‚Äôs slow, but what you‚Äôre doing inside of it. A common culprit of slow loops is modifying a data structure, where each modification generates a copy. See Sections ?? and ?? for more details.‚Ü©Ô∏é Not to be confused with base::Map(), which is considerably more complex. I‚Äôll discuss Map() in Section 1.4.5.‚Ü©Ô∏é Who is highly likely to be future you!‚Ü©Ô∏é In brief, invisible values are only printed if you explicitly request it. This makes them well suited for functions called primarily for their side-effects, as it allows their output to be ignored by default, while still giving an option to capture it. See Section ?? for more details.‚Ü©Ô∏é "],["function-factories.html", "2 Function factories 2.1 Introduction 2.2 Factory fundamentals 2.3 Graphical factories 2.4 Statistical factories 2.5 Function factories + functionals", " 2 Function factories 2.1 Introduction A function factory is a function that makes functions. Here‚Äôs a very simple example: we use a function factory (power1()) to make two child functions (square() and cube()): power1 &lt;- function(exp) { function(x) { x ^ exp } } square &lt;- power1(2) cube &lt;- power1(3) Don‚Äôt worry if this doesn‚Äôt make sense yet, it should by the end of the chapter! I‚Äôll call square() and cube() manufactured functions, but this is just a term to ease communication with other humans: from R‚Äôs perspective they are no different to functions created any other way. square(3) #&gt; [1] 9 cube(3) #&gt; [1] 27 You have already learned about the individual components that make function factories possible: In Section ??, you learned about R‚Äôs first-class functions. In R, you bind a function to a name in the same way as you bind any object to a name: with &lt;-. In Section ??, you learned that a function captures (encloses) the environment in which it is created. In Section ??, you learned that a function creates a new execution environment every time it is run. This environment is usually ephemeral, but here it becomes the enclosing environment of the manufactured function. In this chapter, you‚Äôll learn how the non-obvious combination of these three features leads to the function factory. You‚Äôll also see examples of their usage in visualisation and statistics. Of the three main functional programming tools (functionals, function factories, and function operators), function factories are the least used. Generally, they don‚Äôt tend to reduce overall code complexity but instead partition complexity into more easily digested chunks. Function factories are also an important building block for the very useful function operators, which you‚Äôll learn about in Chapter 3. Outline Section 2.2 begins the chapter with an explanation of how function factories work, pulling together ideas from scoping and environments. You‚Äôll also see how function factories can be used to implement a memory for functions, allowing data to persist across calls. Section 2.3 illustrates the use of function factories with examples from ggplot2. You‚Äôll see two examples of how ggplot2 works with user supplied function factories, and one example of where ggplot2 uses a function factory internally. Section 2.4 uses function factories to tackle three challenges from statistics: understanding the Box-Cox transform, solving maximum likelihood problems, and drawing bootstrap resamples. Section 2.5 shows how you can combine function factories and functionals to rapidly generate a family of functions from data. Prerequisites Make sure you‚Äôre familiar with the contents of Sections ?? (first-class functions), ?? (the function environment), and ?? (execution environments) mentioned above. Function factories only need base R. We‚Äôll use a little rlang to peek inside of them more easily, and we‚Äôll use ggplot2 and scales to explore the use of function factories in visualisation. library(rlang) library(ggplot2) library(scales) 2.2 Factory fundamentals The key idea that makes function factories work can be expressed very concisely: The enclosing environment of the manufactured function is an execution environment of the function factory. It only takes few words to express these big ideas, but it takes a lot more work to really understand what this means. This section will help you put the pieces together with interactive exploration and some diagrams. 2.2.1 Environments Let‚Äôs start by taking a look at square() and cube(): square #&gt; function(x) { #&gt; x ^ exp #&gt; } #&gt; &lt;environment: 0x55d1f510b598&gt; cube #&gt; function(x) { #&gt; x ^ exp #&gt; } #&gt; &lt;bytecode: 0x55d1f5235450&gt; #&gt; &lt;environment: 0x55d1f51541c0&gt; It‚Äôs obvious where x comes from, but how does R find the value associated with exp? Simply printing the manufactured functions is not revealing because the bodies are identical; the contents of the enclosing environment are the important factors. We can get a little more insight by using rlang::env_print(). That shows us that we have two different environments (each of which was originally an execution environment of power1()). The environments have the same parent, which is the enclosing environment of power1(), the global environment. env_print(square) #&gt; &lt;environment: 0x55d1f510b598&gt; #&gt; parent: &lt;environment: global&gt; #&gt; bindings: #&gt; * exp: &lt;dbl&gt; env_print(cube) #&gt; &lt;environment: 0x55d1f51541c0&gt; #&gt; parent: &lt;environment: global&gt; #&gt; bindings: #&gt; * exp: &lt;dbl&gt; env_print() shows us that both environments have a binding to exp, but we want to see its value5. We can do that by first getting the environment of the function, and then extracting the values: fn_env(square)$exp #&gt; [1] 2 fn_env(cube)$exp #&gt; [1] 3 This is what makes manufactured functions behave differently from one another: names in the enclosing environment are bound to different values. 2.2.2 Diagram conventions We can also show these relationships in a diagram: There‚Äôs a lot going on this diagram and some of the details aren‚Äôt that important. We can simplify considerably by using two conventions: Any free floating symbol lives in the global environment. Any environment without an explicit parent inherits from the global environment. This view, which focuses on the environments, doesn‚Äôt show any direct link between cube() and square(). That‚Äôs because the link is the through the body of the function, which is identical for both, but is not shown in this diagram. To finish up, let‚Äôs look at the execution environment of square(10). When square() executes x ^ exp it finds x in the execution environment and exp in its enclosing environment. square(10) #&gt; [1] 100 2.2.3 Forcing evaluation There‚Äôs a subtle bug in power1() caused by lazy evaluation. To see the problem we need to introduce some indirection: x &lt;- 2 square &lt;- power1(x) x &lt;- 3 What should square(2) return? You would hope it returns 4: square(2) #&gt; [1] 8 Unfortunately it doesn‚Äôt because x is only evaluated lazily when square() is run, not when power1() is run. In general, this problem will arise whenever a binding changes in between calling the factory function and calling the manufactured function. This is likely to only happen rarely, but when it does, it will lead to a real head-scratcher of a bug. We can fix this problem by forcing evaluation with force(): power2 &lt;- function(exp) { force(exp) function(x) { x ^ exp } } x &lt;- 2 square &lt;- power2(x) x &lt;- 3 square(2) #&gt; [1] 4 Whenever you create a function factory, make sure every argument is evaluated, using force() as necessary if the argument is only used by the manufactured function. 2.2.4 Stateful functions Function factories also allow you to maintain state across function invocations, which is generally hard to do because of the fresh start principle described in Section ??. There are two things that make this possible: The enclosing environment of the manufactured function is unique and constant. R has a special assignment operator, &lt;&lt;-, which modifies bindings in the enclosing environment. The usual assignment operator, &lt;-, always creates a binding in the current environment. The super assignment operator, &lt;&lt;- rebinds an existing name found in a parent environment. The following example shows how we can combine these ideas to create a function that records how many times it has been called: new_counter &lt;- function() { i &lt;- 0 function() { i &lt;&lt;- i + 1 i } } counter_one &lt;- new_counter() counter_two &lt;- new_counter() When the manufactured function is run i &lt;&lt;- i + 1 will modify i in its enclosing environment. Because manufactured functions have independent enclosing environments, they have independent counts: counter_one() #&gt; [1] 1 counter_one() #&gt; [1] 2 counter_two() #&gt; [1] 1 Stateful functions are best used in moderation. As soon as your function starts managing the state of multiple variables, it‚Äôs better to switch to R6, the topic of Chapter ??. 2.2.5 Garbage collection With most functions, you can rely on the garbage collector to clean up any large temporary objects created inside a function. However, manufactured functions hold on to the execution environment, so you‚Äôll need to explicitly unbind any large temporary objects with rm(). Compare the sizes of g1() and g2() in the example below: f1 &lt;- function(n) { x &lt;- runif(n) m &lt;- mean(x) function() m } g1 &lt;- f1(1e6) lobstr::obj_size(g1) #&gt; 8,013,104 B f2 &lt;- function(n) { x &lt;- runif(n) m &lt;- mean(x) rm(x) function() m } g2 &lt;- f2(1e6) lobstr::obj_size(g2) #&gt; 12,944 B 2.2.6 Exercises The definition of force() is simple: force #&gt; function (x) #&gt; x #&gt; &lt;bytecode: 0x55d1f1342d40&gt; #&gt; &lt;environment: namespace:base&gt; Why is it better to force(x) instead of just x? Base R contains two function factories, approxfun() and ecdf(). Read their documentation and experiment to figure out what the functions do and what they return. Create a function pick() that takes an index, i, as an argument and returns a function with an argument x that subsets x with i. pick(1)(x) # should be equivalent to x[[1]] lapply(mtcars, pick(5)) # should be equivalent to lapply(mtcars, function(x) x[[5]]) Create a function that creates functions that compute the ith central moment of a numeric vector. You can test it by running the following code: m1 &lt;- moment(1) m2 &lt;- moment(2) x &lt;- runif(100) stopifnot(all.equal(m1(x), 0)) stopifnot(all.equal(m2(x), var(x) * 99 / 100)) What happens if you don‚Äôt use a closure? Make predictions, then verify with the code below. i &lt;- 0 new_counter2 &lt;- function() { i &lt;&lt;- i + 1 i } What happens if you use &lt;- instead of &lt;&lt;-? Make predictions, then verify with the code below. new_counter3 &lt;- function() { i &lt;- 0 function() { i &lt;- i + 1 i } } 2.3 Graphical factories We‚Äôll begin our exploration of useful function factories with a few examples from ggplot2. 2.3.1 Labelling One of the goals of the scales package is to make it easy to customise the labels on ggplot2. It provides many functions to control the fine details of axes and legends. The formatter functions6 are a useful class of functions which make it easier to control the appearance of axis breaks. The design of these functions might initially seem a little odd: they all return a function, which you have to call in order to format a number. y &lt;- c(12345, 123456, 1234567) comma_format()(y) #&gt; [1] &quot;12,345&quot; &quot;123,456&quot; &quot;1,234,567&quot; number_format(scale = 1e-3, suffix = &quot; K&quot;)(y) #&gt; [1] &quot;12 K&quot; &quot;123 K&quot; &quot;1 235 K&quot; In other words, the primary interface is a function factory. At first glance, this seems to add extra complexity for little gain. But it enables a nice interaction with ggplot2‚Äôs scales, because they accept functions in the label argument: df &lt;- data.frame(x = 1, y = y) core &lt;- ggplot(df, aes(x, y)) + geom_point() + scale_x_continuous(breaks = 1, labels = NULL) + labs(x = NULL, y = NULL) core core + scale_y_continuous( labels = comma_format() ) core + scale_y_continuous( labels = number_format(scale = 1e-3, suffix = &quot; K&quot;) ) core + scale_y_continuous( labels = scientific_format() ) 2.3.2 Histogram bins A little known feature of geom_histogram() is that the binwidth argument can be a function. This is particularly useful because the function is executed once for each group, which means you can have different binwidths in different facets, which is otherwise not possible. To illustrate this idea, and see where variable binwidth might be useful, I‚Äôm going to construct an example where a fixed binwidth isn‚Äôt great. # construct some sample data with very different numbers in each cell sd &lt;- c(1, 5, 15) n &lt;- 100 df &lt;- data.frame(x = rnorm(3 * n, sd = sd), sd = rep(sd, n)) ggplot(df, aes(x)) + geom_histogram(binwidth = 2) + facet_wrap(~ sd, scales = &quot;free_x&quot;) + labs(x = NULL) Here each facet has the same number of observations, but the variability is very different. It would be nice if we could request that the binwidths vary so we get approximately the same number of observations in each bin. One way to do that is with a function factory that inputs the desired number of bins (n), and outputs a function that takes a numeric vector and returns a binwidth: binwidth_bins &lt;- function(n) { force(n) function(x) { (max(x) - min(x)) / n } } ggplot(df, aes(x)) + geom_histogram(binwidth = binwidth_bins(20)) + facet_wrap(~ sd, scales = &quot;free_x&quot;) + labs(x = NULL) We could use this same pattern to wrap around the base R functions that automatically find the so-called optimal7 binwidth, nclass.Sturges(), nclass.scott(), and nclass.FD(): base_bins &lt;- function(type) { fun &lt;- switch(type, Sturges = nclass.Sturges, scott = nclass.scott, FD = nclass.FD, stop(&quot;Unknown type&quot;, call. = FALSE) ) function(x) { (max(x) - min(x)) / fun(x) } } ggplot(df, aes(x)) + geom_histogram(binwidth = base_bins(&quot;FD&quot;)) + facet_wrap(~ sd, scales = &quot;free_x&quot;) + labs(x = NULL) 2.3.3 ggsave() Finally, I want to show a function factory used internally by ggplot2. ggplot2:::plot_dev() is used by ggsave() to go from a file extension (e.g.¬†png, jpeg etc) to a graphics device function (e.g.¬†png(), jpeg()). The challenge here arises because the base graphics devices have some minor inconsistencies which we need to paper over: Most have filename as first argument but some have file. The width and height of raster graphic devices use pixels units by default, but the vector graphics use inches. A mildly simplified version of plot_dev() is shown below: plot_dev &lt;- function(ext, dpi = 96) { force(dpi) switch(ext, eps = , ps = function(path, ...) { grDevices::postscript( file = filename, ..., onefile = FALSE, horizontal = FALSE, paper = &quot;special&quot; ) }, pdf = function(filename, ...) grDevices::pdf(file = filename, ...), svg = function(filename, ...) svglite::svglite(file = filename, ...), emf = , wmf = function(...) grDevices::win.metafile(...), png = function(...) grDevices::png(..., res = dpi, units = &quot;in&quot;), jpg = , jpeg = function(...) grDevices::jpeg(..., res = dpi, units = &quot;in&quot;), bmp = function(...) grDevices::bmp(..., res = dpi, units = &quot;in&quot;), tiff = function(...) grDevices::tiff(..., res = dpi, units = &quot;in&quot;), stop(&quot;Unknown graphics extension: &quot;, ext, call. = FALSE) ) } plot_dev(&quot;pdf&quot;) #&gt; function(filename, ...) grDevices::pdf(file = filename, ...) #&gt; &lt;bytecode: 0x55d1f5f04150&gt; #&gt; &lt;environment: 0x55d1f62b0728&gt; plot_dev(&quot;png&quot;) #&gt; function(...) grDevices::png(..., res = dpi, units = &quot;in&quot;) #&gt; &lt;bytecode: 0x55d1f5cc6c30&gt; #&gt; &lt;environment: 0x55d1f59505b0&gt; 2.3.4 Exercises Compare and contrast ggplot2::label_bquote() with scales::number_format() 2.4 Statistical factories More motivating examples for function factories come from statistics: The Box-Cox transformation. Bootstrap resampling. Maximum likelihood estimation. All of these examples can be tackled without function factories, but I think function factories are a good fit for these problems and provide elegant solutions. These examples expect some statistical background, so feel free to skip if they don‚Äôt make much sense to you. 2.4.1 Box-Cox transformation The Box-Cox transformation (a type of power transformation) is a flexible transformation often used to transform data towards normality. It has a single parameter, \\(\\lambda\\), which controls the strength of the transformation. We could express the transformation as a simple two argument function: boxcox1 &lt;- function(x, lambda) { stopifnot(length(lambda) == 1) if (lambda == 0) { log(x) } else { (x ^ lambda - 1) / lambda } } But re-formulating as a function factory makes it easy to explore its behaviour with stat_function(): boxcox2 &lt;- function(lambda) { if (lambda == 0) { function(x) log(x) } else { function(x) (x ^ lambda - 1) / lambda } } stat_boxcox &lt;- function(lambda) { stat_function(aes(colour = lambda), fun = boxcox2(lambda), size = 1) } ggplot(data.frame(x = c(0, 5)), aes(x)) + lapply(c(0.5, 1, 1.5), stat_boxcox) + scale_colour_viridis_c(limits = c(0, 1.5)) # visually, log() does seem to make sense as the transformation # for lambda = 0; as values get smaller and smaller, the function # gets close and closer to a log transformation ggplot(data.frame(x = c(0.01, 1)), aes(x)) + lapply(c(0.5, 0.25, 0.1, 0), stat_boxcox) + scale_colour_viridis_c(limits = c(0, 1.5)) In general, this allows you to use a Box-Cox transformation with any function that accepts a unary transformation function: you don‚Äôt have to worry about that function providing ... to pass along additional arguments. I also think that the partitioning of lambda and x into two different function arguments is natural since lambda plays quite a different role than x. 2.4.2 Bootstrap generators Function factories are a useful approach for bootstrapping. Instead of thinking about a single bootstrap (you always need more than one!), you can think about a bootstrap generator, a function that yields a fresh bootstrap every time it is called: boot_permute &lt;- function(df, var) { n &lt;- nrow(df) force(var) function() { col &lt;- df[[var]] col[sample(n, replace = TRUE)] } } boot_mtcars1 &lt;- boot_permute(mtcars, &quot;mpg&quot;) head(boot_mtcars1()) #&gt; [1] 16.4 22.8 22.8 22.8 16.4 19.2 head(boot_mtcars1()) #&gt; [1] 17.8 18.7 30.4 30.4 16.4 21.0 The advantage of a function factory is more clear with a parametric bootstrap where we have to first fit a model. We can do this setup step once, when the factory is called, rather than once every time we generate the bootstrap: boot_model &lt;- function(df, formula) { mod &lt;- lm(formula, data = df) fitted &lt;- unname(fitted(mod)) resid &lt;- unname(resid(mod)) rm(mod) function() { fitted + sample(resid) } } boot_mtcars2 &lt;- boot_model(mtcars, mpg ~ wt) head(boot_mtcars2()) #&gt; [1] 25.0 24.0 21.7 19.2 24.9 16.0 head(boot_mtcars2()) #&gt; [1] 27.4 21.0 20.3 19.4 16.3 21.3 I use rm(mod) because linear model objects are quite large (they include complete copies of the model matrix and input data) and I want to keep the manufactured function as small as possible. 2.4.3 Maximum likelihood estimation The goal of maximum likelihood estimation (MLE) is to find the parameter values for a distribution that make the observed data most likely. To do MLE, you start with a probability function. For example, take the Poisson distribution. If we know \\(\\lambda\\), we can compute the probability of getting a vector \\(\\mathbf{x}\\) of values (\\(x_1\\), \\(x_2\\), ‚Ä¶, \\(x_n\\)) by multiplying the Poisson probability function as follows: \\[ P(\\lambda, \\mathbf{x}) = \\prod_{i=1}^{n} \\frac{\\lambda ^ {x_i} e^{-\\lambda}}{x_i!} \\] In statistics, we almost always work with the log of this function. The log is a monotonic transformation which preserves important properties (i.e.¬†the extrema occur in the same place), but has specific advantages: The log turns a product into a sum, which is easier to work with. Multiplying small numbers yields even smaller numbers, which makes the floating point approximation used by a computer less accurate. Let‚Äôs apply a log transformation to this probability function and simplify it as much as possible: \\[ \\log(P(\\lambda, \\mathbf{x})) = \\sum_{i=1}^{n} \\log(\\frac{\\lambda ^ {x_i} e^{-\\lambda}}{x_i!}) \\] \\[ \\log(P(\\lambda, \\mathbf{x})) = \\sum_{i=1}^{n} \\left( x_i \\log(\\lambda) - \\lambda - \\log(x_i!) \\right) \\] \\[ \\log(P(\\lambda, \\mathbf{x})) = \\sum_{i=1}^{n} x_i \\log(\\lambda) - \\sum_{i=1}^{n} \\lambda - \\sum_{i=1}^{n} \\log(x_i!) \\] \\[ \\log(P(\\lambda, \\mathbf{x})) = \\log(\\lambda) \\sum_{i=1}^{n} x_i - n \\lambda - \\sum_{i=1}^{n} \\log(x_i!) \\] We can now turn this function into an R function. The R function is quite elegant because R is vectorised and, because it‚Äôs a statistical programming language, R comes with built-in functions like the log-factorial (lfactorial()). lprob_poisson &lt;- function(lambda, x) { n &lt;- length(x) (log(lambda) * sum(x)) - (n * lambda) - sum(lfactorial(x)) } Consider this vector of observations: x1 &lt;- c(41, 30, 31, 38, 29, 24, 30, 29, 31, 38) We can use lprob_poisson() to compute the (logged) probability of x1 for different values of lambda. lprob_poisson(10, x1) #&gt; [1] -184 lprob_poisson(20, x1) #&gt; [1] -61.1 lprob_poisson(30, x1) #&gt; [1] -31 So far we‚Äôve been thinking of lambda as fixed and known and the function told us the probability of getting different values of x. But in real-life, we observe x and it is lambda that is unknown. The likelihood is the probability function seen through this lens: we want to find the lambda that makes the observed x the most likely. That is, given x, what value of lambda gives us the highest value of lprob_poisson()? In statistics, we highlight this change in perspective by writing \\(f_{\\mathbf{x}}(\\lambda)\\) instead of \\(f(\\lambda, \\mathbf{x})\\). In R, we can use a function factory. We provide x and generate a function with a single parameter, lambda: ll_poisson1 &lt;- function(x) { n &lt;- length(x) function(lambda) { log(lambda) * sum(x) - n * lambda - sum(lfactorial(x)) } } (We don‚Äôt need force() because length() implicitly forces evaluation of x.) One nice thing about this approach is that we can do some precomputation: any term that only involves x can be computed once in the factory. This is useful because we‚Äôre going to need to call this function many times to find the best lambda. ll_poisson2 &lt;- function(x) { n &lt;- length(x) sum_x &lt;- sum(x) c &lt;- sum(lfactorial(x)) function(lambda) { log(lambda) * sum_x - n * lambda - c } } Now we can use this function to find the value of lambda that maximizes the (log) likelihood: ll1 &lt;- ll_poisson2(x1) ll1(10) #&gt; [1] -184 ll1(20) #&gt; [1] -61.1 ll1(30) #&gt; [1] -31 Rather than trial and error, we can automate the process of finding the best value with optimise(). It will evaluate ll1() many times, using mathematical tricks to narrow in on the largest value as quickly as possible. The results tell us that the highest value is -30.27 which occurs when lambda = 32.1: optimise(ll1, c(0, 100), maximum = TRUE) #&gt; $maximum #&gt; [1] 32.1 #&gt; #&gt; $objective #&gt; [1] -30.3 Now, we could have solved this problem without using a function factory because optimise() passes ... on to the function being optimised. That means we could use the log-probability function directly: optimise(lprob_poisson, c(0, 100), x = x1, maximum = TRUE) #&gt; $maximum #&gt; [1] 32.1 #&gt; #&gt; $objective #&gt; [1] -30.3 The advantage of using a function factory here is fairly small, but there are two niceties: We can precompute some values in the factory, saving computation time in each iteration. The two-level design better reflects the mathematical structure of the underlying problem. These advantages get bigger in more complex MLE problems, where you have multiple parameters and multiple data vectors. 2.4.4 Exercises In boot_model(), why don‚Äôt I need to force the evaluation of df or model? Why might you formulate the Box-Cox transformation like this? boxcox3 &lt;- function(x) { function(lambda) { if (lambda == 0) { log(x) } else { (x ^ lambda - 1) / lambda } } } Why don‚Äôt you need to worry that boot_permute() stores a copy of the data inside the function that it generates? How much time does ll_poisson2() save compared to ll_poisson1()? Use bench::mark() to see how much faster the optimisation occurs. How does changing the length of x change the results? 2.5 Function factories + functionals To finish off the chapter, I‚Äôll show how you might combine functionals and function factories to turn data into many functions. The following code creates many specially named power functions by iterating over a list of arguments: names &lt;- list( square = 2, cube = 3, root = 1/2, cuberoot = 1/3, reciprocal = -1 ) funs &lt;- purrr::map(names, power1) funs$root(64) #&gt; [1] 8 funs$root #&gt; function(x) { #&gt; x ^ exp #&gt; } #&gt; &lt;bytecode: 0x55d1f5235450&gt; #&gt; &lt;environment: 0x55d1f7d01378&gt; This idea extends in a straightforward way if your function factory takes two (replace map() with map2()) or more (replace with pmap()) arguments. One downside of the current construction is that you have to prefix every function call with funs$. There are three ways to eliminate this additional syntax: For a very temporary effect, you can use with(): with(funs, root(100)) #&gt; [1] 10 I recommend this because it makes it very clear when code is being executed in a special context and what that context is. For a longer effect, you can attach() the functions to the search path, then detach() when you‚Äôre done: attach(funs) #&gt; The following objects are masked _by_ .GlobalEnv: #&gt; #&gt; cube, square root(100) #&gt; [1] 10 detach(funs) You‚Äôve probably been told to avoid using attach(), and that‚Äôs generally good advice. However, the situation is a little different to the usual because we‚Äôre attaching a list of functions, not a data frame. It‚Äôs less likely that you‚Äôll modify a function than a column in a data frame, so the some of the worst problems with attach() don‚Äôt apply. Finally, you could copy the functions to the global environment with env_bind() (you‚Äôll learn about !!! in Section ??). This is mostly permanent: rlang::env_bind(globalenv(), !!!funs) root(100) #&gt; [1] 10 You can later unbind those same names, but there‚Äôs no guarantee that they haven‚Äôt been rebound in the meantime, and you might be deleting an object that someone else created. rlang::env_unbind(globalenv(), names(funs)) You‚Äôll learn an alternative approach to the same problem in Section ??. Instead of using a function factory, you could construct the function with quasiquotation. This requires additional knowledge, but generates functions with readable bodies, and avoids accidentally capturing large objects in the enclosing scope. We use that idea in Section ?? when we work on tools for generating HTML from R. 2.5.1 Exercises Which of the following commands is equivalent to with(x, f(z))? x$f(x$z). f(x$z). x$f(z). f(z). It depends. Compare and contrast the effects of env_bind() vs.¬†attach() for the following code. funs &lt;- list( mean = function(x) mean(x, na.rm = TRUE), sum = function(x) sum(x, na.rm = TRUE) ) attach(funs) #&gt; The following objects are masked from package:base: #&gt; #&gt; mean, sum mean &lt;- function(x) stop(&quot;Hi!&quot;) detach(funs) env_bind(globalenv(), !!!funs) mean &lt;- function(x) stop(&quot;Hi!&quot;) env_unbind(globalenv(), names(funs)) A future version of env_print() is likely to do better at summarising the contents so you don‚Äôt need this step.‚Ü©Ô∏é It‚Äôs an unfortunate accident of history that scales uses function suffixes instead of function prefixes. That‚Äôs because it was written before I understood the autocomplete advantages to using common prefixes instead of common suffixes.‚Ü©Ô∏é ggplot2 doesn‚Äôt expose these functions directly because I don‚Äôt think the definition of optimality needed to make the problem mathematically tractable is a good match to the actual needs of data exploration.‚Ü©Ô∏é "],["function-operators.html", "3 Function operators 3.1 Introduction 3.2 Existing function operators 3.3 Case study: Creating your own function operators", " 3 Function operators 3.1 Introduction In this chapter, you‚Äôll learn about function operators. A function operator is a function that takes one (or more) functions as input and returns a function as output. The following code shows a simple function operator, chatty(). It wraps a function, making a new function that prints out its first argument. You might create a function like this because it gives you a window to see how functionals, like map_int(), work. chatty &lt;- function(f) { force(f) function(x, ...) { res &lt;- f(x, ...) cat(&quot;Processing &quot;, x, &quot;\\n&quot;, sep = &quot;&quot;) res } } f &lt;- function(x) x ^ 2 s &lt;- c(3, 2, 1) purrr::map_dbl(s, chatty(f)) #&gt; Processing 3 #&gt; Processing 2 #&gt; Processing 1 #&gt; [1] 9 4 1 Function operators are closely related to function factories; indeed they‚Äôre just a function factory that takes a function as input. Like factories, there‚Äôs nothing you can‚Äôt do without them, but they often allow you to factor out complexity in order to make your code more readable and reusable. Function operators are typically paired with functionals. If you‚Äôre using a for-loop, there‚Äôs rarely a reason to use a function operator, as it will make your code more complex for little gain. If you‚Äôre familiar with Python, decorators is just another name for function operators. Outline Section 3.2 introduces you to two extremely useful existing function operators, and shows you how to use them to solve real problems. Section 3.3 works through a problem amenable to solution with function operators: downloading many web pages. Prerequisites Function operators are a type of function factory, so make sure you‚Äôre familiar with at least Section ?? before you go on. We‚Äôll use purrr for a couple of functionals that you learned about in Chapter 1, and some function operators that you‚Äôll learn about below. We‚Äôll also use the memoise package [@memoise] for the memoise() operator. library(purrr) library(memoise) 3.2 Existing function operators There are two very useful function operators that will both help you solve common recurring problems, and give you a sense for what function operators can do: purrr::safely() and memoise::memoise(). 3.2.1 Capturing errors with purrr::safely() One advantage of for-loops is that if one of the iterations fails, you can still access all the results up to the failure: x &lt;- list( c(0.512, 0.165, 0.717), c(0.064, 0.781, 0.427), c(0.890, 0.785, 0.495), &quot;oops&quot; ) out &lt;- rep(NA_real_, length(x)) for (i in seq_along(x)) { out[[i]] &lt;- sum(x[[i]]) } #&gt; Error in sum(x[[i]]): invalid &#39;type&#39; (character) of argument out #&gt; [1] 1.39 1.27 2.17 NA If you do the same thing with a functional, you get no output, making it hard to figure out where the problem lies: map_dbl(x, sum) #&gt; Error in .Primitive(&quot;sum&quot;)(..., na.rm = na.rm): invalid &#39;type&#39; (character) of argument purrr::safely() provides a tool to help with this problem. safely() is a function operator that transforms a function to turn errors into data. (You can learn the basic idea that makes it work in Section ??.) Let‚Äôs start by taking a look at it outside of map_dbl(): safe_sum &lt;- safely(sum) safe_sum #&gt; function (...) #&gt; capture_error(.f(...), otherwise, quiet) #&gt; &lt;bytecode: 0x55dd0415f418&gt; #&gt; &lt;environment: 0x55dd0415ef80&gt; Like all function operators, safely() takes a function and returns a wrapped function which we can call as usual: str(safe_sum(x[[1]])) #&gt; List of 2 #&gt; $ result: num 1.39 #&gt; $ error : NULL str(safe_sum(x[[4]])) #&gt; List of 2 #&gt; $ result: NULL #&gt; $ error :List of 2 #&gt; ..$ message: chr &quot;invalid &#39;type&#39; (character) of argument&quot; #&gt; ..$ call : language .Primitive(&quot;sum&quot;)(..., na.rm = na.rm) #&gt; ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; You can see that a function transformed by safely() always returns a list with two elements, result and error. If the function runs successfully, error is NULL and result contains the result; if the function fails, result is NULL and error contains the error. Now lets use safely() with a functional: out &lt;- map(x, safely(sum)) str(out) #&gt; List of 4 #&gt; $ :List of 2 #&gt; ..$ result: num 1.39 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: num 1.27 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: num 2.17 #&gt; ..$ error : NULL #&gt; $ :List of 2 #&gt; ..$ result: NULL #&gt; ..$ error :List of 2 #&gt; .. ..$ message: chr &quot;invalid &#39;type&#39; (character) of argument&quot; #&gt; .. ..$ call : language .Primitive(&quot;sum&quot;)(..., na.rm = na.rm) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; The output is in a slightly inconvenient form, since we have four lists, each of which is a list containing the result and the error. We can make the output easier to use by turning it ‚Äúinside-out‚Äù with purrr::transpose(), so that we get a list of results and a list of errors: out &lt;- transpose(map(x, safely(sum))) str(out) #&gt; List of 2 #&gt; $ result:List of 4 #&gt; ..$ : num 1.39 #&gt; ..$ : num 1.27 #&gt; ..$ : num 2.17 #&gt; ..$ : NULL #&gt; $ error :List of 4 #&gt; ..$ : NULL #&gt; ..$ : NULL #&gt; ..$ : NULL #&gt; ..$ :List of 2 #&gt; .. ..$ message: chr &quot;invalid &#39;type&#39; (character) of argument&quot; #&gt; .. ..$ call : language .Primitive(&quot;sum&quot;)(..., na.rm = na.rm) #&gt; .. ..- attr(*, &quot;class&quot;)= chr [1:3] &quot;simpleError&quot; &quot;error&quot; &quot;condition&quot; Now we can easily find the results that worked, or the inputs that failed: ok &lt;- map_lgl(out$error, is.null) ok #&gt; [1] TRUE TRUE TRUE FALSE x[!ok] #&gt; [[1]] #&gt; [1] &quot;oops&quot; out$result[ok] #&gt; [[1]] #&gt; [1] 1.39 #&gt; #&gt; [[2]] #&gt; [1] 1.27 #&gt; #&gt; [[3]] #&gt; [1] 2.17 You can use this same technique in many different situations. For example, imagine you‚Äôre fitting a generalised linear model (GLM) to a list of data frames. GLMs can sometimes fail because of optimisation problems, but you still want to be able to try to fit all the models, and later look back at those that failed: fit_model &lt;- function(df) { glm(y ~ x1 + x2 * x3, data = df) } models &lt;- transpose(map(datasets, safely(fit_model))) ok &lt;- map_lgl(models$error, is.null) # which data failed to converge? datasets[!ok] # which models were successful? models[ok] I think this is a great example of the power of combining functionals and function operators: safely() lets you succinctly express what you need to solve a common data analysis problem. purrr comes with three other function operators in a similar vein: possibly(): returns a default value when there‚Äôs an error. It provides no way to tell if an error occured or not, so it‚Äôs best reserved for cases when there‚Äôs some obvious sentinel value (like NA). quietly(): turns output, messages, and warning side-effects into output, message, and warning components of the output. auto_browser(): automatically executes browser() inside the function when there‚Äôs an error. See their documentation for more details. 3.2.2 Caching computations with memoise::memoise() Another handy function operator is memoise::memoise(). It memoises a function, meaning that the function will remember previous inputs and return cached results. Memoisation is an example of the classic computer science tradeoff of memory versus speed. A memoised function can run much faster, but because it stores all of the previous inputs and outputs, it uses more memory. Let‚Äôs explore this idea with a toy function that simulates an expensive operation: slow_function &lt;- function(x) { Sys.sleep(1) x * 10 * runif(1) } system.time(print(slow_function(1))) #&gt; [1] 0.808 #&gt; user system elapsed #&gt; 0 0 1 system.time(print(slow_function(1))) #&gt; [1] 8.34 #&gt; user system elapsed #&gt; 0.003 0.000 1.004 When we memoise this function, it‚Äôs slow when we call it with new arguments. But when we call it with arguments that it‚Äôs seen before it‚Äôs instantaneous: it retrieves the previous value of the computation. fast_function &lt;- memoise::memoise(slow_function) system.time(print(fast_function(1))) #&gt; [1] 6.01 #&gt; user system elapsed #&gt; 0.001 0.000 1.002 system.time(print(fast_function(1))) #&gt; [1] 6.01 #&gt; user system elapsed #&gt; 0.017 0.000 0.017 A relatively realistic use of memoisation is computing the Fibonacci series. The Fibonacci series is defined recursively: the first two values are defined by convention, \\(f(0) = 0\\), \\(f(1) = 1\\), and then \\(f(n) = f(n - 1) + f(n - 2)\\) (for any positive integer). A naive version is slow because, for example, fib(10) computes fib(9) and fib(8), and fib(9) computes fib(8) and fib(7), and so on. fib &lt;- function(n) { if (n &lt; 2) return(1) fib(n - 2) + fib(n - 1) } system.time(fib(23)) #&gt; user system elapsed #&gt; 0.047 0.000 0.047 system.time(fib(24)) #&gt; user system elapsed #&gt; 0.08 0.00 0.08 Memoising fib() makes the implementation much faster because each value is computed only once: fib2 &lt;- memoise::memoise(function(n) { if (n &lt; 2) return(1) fib2(n - 2) + fib2(n - 1) }) system.time(fib2(23)) #&gt; user system elapsed #&gt; 0.007 0.000 0.007 And future calls can rely on previous computations: system.time(fib2(24)) #&gt; user system elapsed #&gt; 0.001 0.000 0.001 This is an example of dynamic programming, where a complex problem can be broken down into many overlapping subproblems, and remembering the results of a subproblem considerably improves performance. Think carefully before memoising a function. If the function is not pure, i.e.¬†the output does not depend only on the input, you will get misleading and confusing results. I created a subtle bug in devtools because I memoised the results of available.packages(), which is rather slow because it has to download a large file from CRAN. The available packages don‚Äôt change that frequently, but if you have an R process that‚Äôs been running for a few days, the changes can become important, and because the problem only arose in long-running R processes, the bug was very painful to find. 3.2.3 Exercises Base R provides a function operator in the form of Vectorize(). What does it do? When might you use it? Read the source code for possibly(). How does it work? Read the source code for safely(). How does it work? 3.3 Case study: Creating your own function operators meomoise() and safely() are very useful but also quite complex. In this case study you‚Äôll learn how to create your own simpler function operators. Imagine you have a named vector of URLs and you‚Äôd like to download each one to disk. That‚Äôs pretty simple with walk2() and file.download(): urls &lt;- c( &quot;adv-r&quot; = &quot;https://adv-r.hadley.nz&quot;, &quot;r4ds&quot; = &quot;http://r4ds.had.co.nz/&quot; # and many many more ) path &lt;- paste(tempdir(), names(urls), &quot;.html&quot;) walk2(urls, path, download.file, quiet = TRUE) This approach is fine for a handful of URLs, but as the vector gets longer, you might want to add a couple more features: Add a small delay between each request to avoid hammering the server. Display a . every few URLs so that we know that the function is still working. It‚Äôs relatively easy to add these extra features if we‚Äôre using a for loop: for(i in seq_along(urls)) { Sys.sleep(0.1) if (i %% 10 == 0) cat(&quot;.&quot;) download.file(urls[[i]], paths[[i]]) } I think this for loop is suboptimal because it interleaves different concerns: pausing, showing progress, and downloading. This makes the code harder to read, and it makes it harder to reuse the components in new situations. Instead, let‚Äôs see if we can use function operators to extract out pausing and showing progress and make them reusable. First, let‚Äôs write a function operator that adds a small delay. I‚Äôm going to call it delay_by() for reasons that will be more clear shortly, and it has two arguments: the function to wrap, and the amount of delay to add. The actual implementation is quite simple. The main trick is forcing evaluation of all arguments as described in Section 2.2.5, because function operators are a special type of function factory: delay_by &lt;- function(f, amount) { force(f) force(amount) function(...) { Sys.sleep(amount) f(...) } } system.time(runif(100)) #&gt; user system elapsed #&gt; 0 0 0 system.time(delay_by(runif, 0.1)(100)) #&gt; user system elapsed #&gt; 0.0 0.0 0.1 And we can use it with the original walk2(): walk2(urls, path, delay_by(download.file, 0.1), quiet = TRUE) Creating a function to display the occasional dot is a little harder, because we can no longer rely on the index from the loop. We could pass the index along as another argument, but that breaks encapsulation: a concern of the progress function now becomes a problem that the higher level wrapper needs to handle. Instead, we‚Äôll use another function factory trick (from Section 2.2.4), so that the progress wrapper can manage its own internal counter: dot_every &lt;- function(f, n) { force(f) force(n) i &lt;- 0 function(...) { i &lt;&lt;- i + 1 if (i %% n == 0) cat(&quot;.&quot;) f(...) } } walk(1:100, runif) walk(1:100, dot_every(runif, 10)) #&gt; .......... Now we can express our original for loop as: walk2( urls, path, dot_every(delay_by(download.file, 0.1), 10), quiet = TRUE ) This is starting to get a little hard to read because we are composing many function calls, and the arguments are getting spread out. One way to resolve that is to use the pipe: walk2( urls, path, download.file %&gt;% dot_every(10) %&gt;% delay_by(0.1), quiet = TRUE ) The pipe works well here because I‚Äôve carefully chosen the function names to yield an (almost) readable sentence: take download.file then (add) a dot every 10 iterations, then delay by 0.1s. The more clearly you can express the intent of your code through function names, the more easily others (including future you!) can read and understand the code. 3.3.1 Exercises Weigh the pros and cons of download.file %&gt;% dot_every(10) %&gt;% delay_by(0.1) versus download.file %&gt;% delay_by(0.1) %&gt;% dot_every(10). Should you memoise file.download()? Why or why not? Create a function operator that reports whenever a file is created or deleted in the working directory, using dir() and setdiff(). What other global function effects might you want to track? Write a function operator that logs a timestamp and message to a file every time a function is run. Modify delay_by() so that instead of delaying by a fixed amount of time, it ensures that a certain amount of time has elapsed since the function was last called. That is, if you called g &lt;- delay_by(1, f); g(); Sys.sleep(2); g() there shouldn‚Äôt be an extra delay. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
